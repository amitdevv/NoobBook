# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Build & Run Commands

### macOS / Linux (Recommended)
```bash
# First time setup (creates venv, installs all dependencies)
bin/setup

# Run both backend and frontend
bin/dev

# Options
bin/dev --backend-only    # Only Flask server (http://localhost:5001)
bin/dev --frontend-only   # Only Vite server (http://localhost:5173)
bin/dev --install         # Update deps before starting
```

### Windows
```bash
# First time setup
cd backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt

cd ../frontend
npm install

# Run both servers
python start.py

# Stop servers
python stop.py
```

### Other Commands
```bash
npm run build                  # Production build (frontend)
npm run lint                   # ESLint (frontend)
```

### System Dependencies (Required)
```bash
# macOS
brew install libreoffice ffmpeg
npx playwright install

# Ubuntu/Debian
sudo apt install libreoffice ffmpeg
npx playwright install
```

### Docker with Self-Hosted Supabase (macOS)
For running the full stack with self-hosted Supabase on macOS, see **[docker/MAC_SETUP.md](docker/MAC_SETUP.md)**.

Quick start:
```bash
cd docker
cp .env.example .env
# Edit .env with your API keys
./setup.sh
```

This starts 16 containers including PostgreSQL, Supabase services, and MinIO for storage.

### Testing
```bash
# Backend tests
cd backend && pytest

# Single test file
pytest tests/test_sources.py

# Single test
pytest tests/test_sources.py::test_upload_pdf -v
```

### Environment Variables
Create `backend/.env` with:
```bash
# Required
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
PINECONE_API_KEY=...
PINECONE_INDEX_NAME=...

# Optional
ELEVENLABS_API_KEY=...          # Audio features
TAVILY_API_KEY=...              # Web search fallback
GOOGLE_CLIENT_ID=...            # Google Drive import
GOOGLE_CLIENT_SECRET=...
JIRA_DOMAIN=...                 # Jira integration (https://your-company.atlassian.net)
JIRA_EMAIL=...                  # Jira user email
JIRA_API_KEY=...                # Jira API token
NOTION_API_KEY=...              # Notion integration
ANTHROPIC_TIER=1                # 1-4, controls rate limits
```

### Claude Model
Always use `claude-sonnet-4-5-20250929` as the default model for Claude API calls.

## PROJECT PURPOSE

**NoobBook is an open-source NotebookLM alternative. NotebookLM, but smarter.**

Website: [noobbooklm.com](https://noobbooklm.com)

### What We Focus On
- LLM API integration patterns (Claude, OpenAI, etc.)
- Prompt engineering and system prompt design
- Tool use and agentic loops
- RAG (Retrieval Augmented Generation) with embeddings
- Multi-modal AI (vision, audio transcription, text-to-speech)
- Studio content generation (audio, video, documents, design)

### Current Scope
- Self-hosted Supabase (PostgreSQL + Storage + Auth)
- Multi-user with team collaboration support
- RLS policies for data isolation per user
- API key management via UI

### Code Philosophy
- Keep it simple and readable
- Add comments explaining LLM concepts where helpful
- Each service should have a clear, single purpose
- Avoid over-engineering

## IMPORTANT RULES TO FOLLOW

### Code Quality & Structure
- Follow DRY â€” extract repeated logic into reusable functions/components - Think of this as the learning session will be attended by the Founder of Python! He should be amazed not disgusted
- Keep code modular â€” one component/function per file when it exceeds 100 lines
- Prefer composition over inheritance in React components

### Frontend Rules (React + Vite + shadcn + Tailwind)
- **Always check shadcn/ui first** â€” before creating custom components, search if shadcn already provides it
- Use shadcn components via: `npx shadcn@latest add [component-name]`
- Tailwind for all styling â€” no inline styles or separate CSS files unless absolutely necessary
- Toast notifications use the custom hook from `./ui/toast` (not `../hooks/use-toast`)

### Design System
- **Icons**: Use Phosphor Icons (`@phosphor-icons/react`) â€” NOT Lucide React
- **Colors**: Amber-600 primary (`#D97706`), Stone-800 text, warm cream background
- **Full spec**: See `frontend/DESIGN_SYSTEM.md` for complete reference

### Backend Rules (Python Flask)
- Follow PEP 8 style guidelines
- Use type hints for all function parameters and return values
- Organize routes in blueprints when they exceed 5 endpoints

### Documentation & Comments
- **Add explanatory comments** for LLM-related code â€” explain WHY, not just WHAT
- Document agent architectures and prompt engineering decisions
- Add docstrings to Python functions and classes

### When In Doubt â€” ASK
- If approach is unclear, ask before implementing

## Project Overview

NoobBook - An open-source NotebookLM alternative. Full-stack web application with React frontend and Flask backend.

## Application Views

The application has exactly **two core views**:

### View 1: Dashboard
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NoobBook                                             [ App Settings ]      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚         +           â”‚    â”‚  Project Name       â”‚                       â”‚
â”‚   â”‚                     â”‚    â”‚  Description...     â”‚                       â”‚
â”‚   â”‚  Create New Project â”‚    â”‚                     â”‚                       â”‚
â”‚   â”‚                     â”‚    â”‚  Last opened: date  â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Session 3: The Complete AI Tool     â”‚  Previous Sessions & Resources      â”‚
â”‚                                       â”‚                                     â”‚
â”‚  Tags: AI Chat, RAG, Image Gen,      â”‚  - Session 1: API Basics            â”‚
â”‚  Video Gen, Realtime Transcription,  â”‚  - Session 2: Chat, Memory & Agents â”‚
â”‚  Memories, Subagents, Web Search     â”‚  - Course Code Repository           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Purpose**: Project management (create, open, delete projects) + course learning resources

### View 2: Project Workspace (3-Panel Layout)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â† Project Name                        [ Memory ] [ Project Settings ] [ + New ]     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SOURCES          â”‚  CHAT                               â”‚  STUDIO                    â”‚
â”‚                   â”‚                                     â”‚                            â”‚
â”‚  [Search...]      â”‚  Chat Name â–¼               [8/8]   â”‚  Generate content from     â”‚
â”‚  [+ Add sources]  â”‚  Ask questions about sources...     â”‚  your sources              â”‚
â”‚                   â”‚                                     â”‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  DOCUMENTS                 â”‚
â”‚  â”‚ ðŸ“„ PDF      â”‚  â”‚  â”‚ User: "question here?"      â”‚   â”‚  â€¢ Generate Presentation   â”‚
â”‚  â”‚ ðŸ–¼ï¸ Image    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â€¢ Generate PRD / Docs     â”‚
â”‚  â”‚ ðŸŽµ Audio    â”‚  â”‚                                     â”‚  â€¢ Generate To-Do List     â”‚
â”‚  â”‚ ðŸ”— Link     â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                            â”‚
â”‚  â”‚ ðŸ“º YouTube  â”‚  â”‚  â”‚ NoobBook:                   â”‚   â”‚  COMMUNICATION             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ Response with citationsÂ¹Â²Â³  â”‚   â”‚  â€¢ Draft Team Email        â”‚
â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â€¢ Draft Stakeholder Email â”‚
â”‚                   â”‚                                     â”‚                            â”‚
â”‚  8/100 sources    â”‚  [ðŸŽ¤] Ask about your sources...    â”‚  MEDIA                     â”‚
â”‚  5.2 MB total     â”‚                                     â”‚  â€¢ Audio Overview          â”‚
â”‚                   â”‚  Click mic to speak, or type       â”‚  â€¢ Video Overview          â”‚
â”‚                   â”‚                                     â”‚                            â”‚
â”‚                   â”‚                                     â”‚  ANALYSIS                  â”‚
â”‚                   â”‚                                     â”‚  â€¢ Generate Mind Map       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Purpose**: Where all AI features live
- **Sources Panel**: Multi-modal ingestion (PDF, DOCX, PPTX, images, audio, YouTube, URLs)
- **Chat Panel**: RAG-based Q&A with citations, voice input, conversation history
- **Studio Panel**: Content generation features (planned)

### Data Structure

#### Supabase Database (PostgreSQL)
All user data is stored in Supabase with Row-Level Security (RLS) for multi-user isolation:

| Table | Purpose |
|-------|---------|
| `users` | User accounts, global memory, settings, google_tokens |
| `projects` | Project metadata, custom prompts, costs, project memory |
| `sources` | Source metadata, status, file paths, token counts |
| `chats` | Chat containers and metadata |
| `messages` | Chat messages with JSONB content |
| `chunks` | RAG text chunks with metadata |
| `studio_signals` | AI-emitted signals for studio features |
| `background_tasks` | Async task tracking |
| `brand_assets` | Brand asset metadata |
| `brand_config` | Project brand configuration |

#### Supabase Storage (S3-compatible)
| Bucket | Purpose |
|--------|---------|
| `raw-files` | Original uploaded files (PDFs, DOCX, images, audio) |
| `processed-files` | Extracted/processed text content |
| `chunks` | Text chunks for RAG |
| `studio-outputs` | Generated content (audio, video, PDFs) |
| `brand-assets` | Brand logos, icons, fonts |

#### Local Files (Configuration & Debug Only)
```
data/
â”œâ”€â”€ prompts/                      # System prompt configurations (not user data)
â”‚   â”œâ”€â”€ default_prompt.json
â”‚   â”œâ”€â”€ pdf_extraction_prompt.json
â”‚   â”œâ”€â”€ memory_prompt.json
â”‚   â””â”€â”€ ...                       # Other prompt configs
â””â”€â”€ projects/{id}/agents/         # Debug logs only (optional)
    â””â”€â”€ web_agent/{execution_id}.json

app/services/tools/               # Tool definitions (JSON schemas)
â”œâ”€â”€ chat_tools/
â”‚   â”œâ”€â”€ source_search_tool.json
â”‚   â””â”€â”€ memory_tool.json
â”œâ”€â”€ pdf_tools/, pptx_tools/, image_tools/
â””â”€â”€ web_agent/
    â”œâ”€â”€ web_search.json
    â”œâ”€â”€ web_fetch.json
    â””â”€â”€ return_search_result.json
```

### API Endpoints
Base URL: `http://localhost:5001/api/v1`

**Projects**: GET/POST `/projects`, GET/PUT/DELETE `/projects/{id}`, GET `/projects/{id}/costs`, GET `/projects/{id}/memory`

**Chats**:
- `GET/POST /projects/{id}/chats` - List/create chats
- `GET/PUT/DELETE /projects/{id}/chats/{chat_id}` - Chat operations
- `POST /projects/{id}/chats/{chat_id}/messages` - Send message (calls Claude API)
- `GET /projects/{id}/prompt` - Get project's system prompt
- `GET /prompts/default` - Get global default prompt

**Sources**:
- `GET /projects/{id}/sources` - List all sources
- `POST /projects/{id}/sources` - Upload file (multipart/form-data)
- `POST /projects/{id}/sources/url` - Add URL source
- `POST /projects/{id}/sources/text` - Add pasted text source
- `PUT /projects/{id}/sources/{source_id}` - Update source (name, description, active)
- `DELETE /projects/{id}/sources/{source_id}` - Delete source
- `POST /projects/{id}/sources/{source_id}/cancel` - Cancel processing
- `POST /projects/{id}/sources/{source_id}/retry` - Retry/start processing

**Settings**: `GET/POST /settings/api-keys`, `DELETE /settings/api-keys/{id}`, `POST /settings/api-keys/validate`

**Google Drive**:
- `GET /google/status` - Check if configured and connected
- `GET /google/auth` - Get OAuth authorization URL
- `GET /google/callback` - Handle OAuth callback (redirects to frontend)
- `POST /google/disconnect` - Remove stored tokens
- `GET /google/files` - List files (supports `folder_id`, `page_size`, `page_token`)
- `POST /projects/{id}/sources/google-import` - Import file to sources

**Transcription**:
- `GET /transcription/config` - Get ElevenLabs WebSocket URL with single-use token
- `GET /transcription/status` - Check if ElevenLabs is configured

## Google Drive Integration

Import files from Google Drive. OAuth 2.0 flow with `drive.readonly` scope. Tokens stored per-user in Supabase `users.google_tokens` column, auto-refresh on expiry. Google Workspace exports: Docsâ†’DOCX, Sheetsâ†’CSV, Slidesâ†’PPTX.

**Setup**: Create OAuth credentials in Google Cloud Console, add redirect URI `http://localhost:5001/api/v1/google/callback`.

**Multi-user Support**: Each user has their own Google Drive connection. The OAuth state parameter carries user_id for proper token association.

## Voice Input (ElevenLabs)

Real-time speech-to-text via ElevenLabs WebSocket. Backend generates single-use tokens (15 min expiry), frontend connects directly. Audio captured via AudioWorklet, converted to 16-bit PCM base64. API key stays server-side.

## Source Processing Pipeline

Status flow: `uploaded â†’ processing â†’ [embedding] â†’ ready` (embedding only if token count > 2500).

### Processed Output Format

All processors use `build_processed_output()` from `app/utils/text/processed_output.py` for consistent output format:

```
# Extracted from {TYPE} document: {source_name}
# Type: {TYPE}
# Total pages: {N}
# Processed at: {timestamp}
# {metadata_key}: {value}
# ...
# token_count: {count or "200k+"}
# ---

=== {TYPE} PAGE 1 of N ===

{page content}

=== {TYPE} PAGE 2 of N ===

{page content}
```

**Key Design Decisions:**
- **Sources with logical pages** (PDF, PPTX, Image batch): Preserve real page/slide structure
- **Sources without logical pages** (TEXT, DOCX, AUDIO, LINK, YOUTUBE): Single page marker `=== TYPE PAGE 1 of 1 ===`
- **Token-based chunking** handles all splitting for embeddings (~200 tokens per chunk)
- Each page extraction is **self-contained** with context from surrounding pages included

### Page Markers
All extracted content uses page markers: `=== {TYPE} PAGE 1 of N ===`
Types: PDF, TEXT, DOCX, PPTX, AUDIO, IMAGE, LINK, YOUTUBE

### Source Types & AI Patterns

| Type | Service | AI Method | Pages |
|------|---------|-----------|-------|
| **PDF** | `ai_services/pdf_service.py` | Batched vision extraction (5 pages/batch, parallel ThreadPool). `submit_page_extraction` tool. | Real pages |
| **PPTX** | `ai_services/pptx_service.py` | Same pattern as PDF - slides as images in batches, `submit_slide_extraction` tool. | Real slides |
| **Image** | `ai_services/image_service.py` | Single Claude vision call with `submit_image_extraction` tool. | 1 per image |
| **URL** | `ai_agents/web_agent_service.py` | Agentic loop with `web_fetch`, `tavily_search` tools. | Single page |
| **DOCX** | `utils/docx_utils.py` | No AI - python-docx extraction | Single page |
| **Audio** | `integrations/elevenlabs/audio_service.py` | No AI - ElevenLabs Scribe v1 transcription | Single page |
| **Text** | `source_processing_service.py` | No AI - direct file read | Single page |
| **YouTube** | `integrations/youtube/youtube_service.py` | No AI - youtube-transcript-api | Single page |

**Design**: Raw files preserved on error (retry without re-upload). Processing runs in background threads. Tool-based extraction ensures structured output.

### Text Utilities (`app/utils/text/`)

Modular text processing for the RAG pipeline:

| Module | Purpose |
|--------|---------|
| `cleaning.py` | Text cleaning for embeddings (normalize whitespace, remove noise) |
| `page_markers.py` | Standardized page marker format and regex patterns |
| `processed_output.py` | `build_processed_output()` for consistent file headers |
| `chunking.py` | Token-based chunking (~200 tokens) for embeddings |

**Note**: `splitting.py` was removed - artificial page splitting is no longer used. Token-based chunking handles all splitting for embeddings.

### Token Counting (Hybrid Approach)

Token counting uses **tiktoken** (local) for speed, with Claude API available for exact counts when needed.

**Why tiktoken?** Chunking calls `count_tokens()` thousands of times (per page, per sentence, per word for long sentences). API calls would take minutes due to network latency. tiktoken is local and instant.

```python
# embedding_utils.py
count_tokens(text)      # Uses tiktoken (fast, local) - for chunking operations
count_tokens_api(text)  # Uses Claude API (accurate, slower) - for billing/quota
```

tiktoken uses `cl100k_base` encoding which closely matches Claude's tokenizer (within ~5% accuracy - good enough for chunking).

## Web Agent Architecture

**Agentic loop pattern** for URL content extraction. Loop: Claude calls tools â†’ execute â†’ return result â†’ repeat until termination.

**Tool Types**:
- **Server Tools**: Claude handles execution (web_fetch, web_search) - results come in response
- **Client Tools**: We execute via `web_agent_executor.py` (tavily_search)
- **Termination Tool**: `return_search_result` signals completion

Execution logs saved to `data/projects/{id}/agents/web_agent/{execution_id}.json` for debugging.

## Source Summaries

AI-generated summaries (150-200 tokens via Haiku) help chat AI understand documents at a glance. Summaries included in system prompt via `context_loader.py`. For large sources, samples 8 evenly distributed chunks.

## Chat Auto-Naming

Background task generates 1-5 word title via Haiku after first message. Non-blocking - chat response returns immediately. Manual rename available via ChatList UI.

## Main Chat Architecture

**RAG agentic loop** for source-aware conversations. System prompt includes dynamic source context + memory via `context_loader.py`.

### Tool Use Loop
```
User message â†’ Claude API (with search_sources, store_memory tools)
    â†“
tool_use? â†’ Yes: Execute tool â†’ Store tool_use + tool_result â†’ Loop back
          â†’ No:  Store final text â†’ Return to user
```

### Source Search (Hybrid Search)

The `search_sources` tool uses a smart hybrid search strategy based on source size:

**Tool Schema** (`source_search_tool.json`):
```json
{
  "source_id": "required - the source to search (from available sources)",
  "keywords": ["optional", "array"],  // 1-2 word terms for local text search
  "query": "optional string"           // semantic search phrase for Pinecone
}
```

**Search Strategy** (`source_search_executor.py`):
```
if source.token_count < 1000:
    â†’ Return ALL chunks (no search needed)
else:
    â†’ Local keyword search (fuzzy matching via difflib)
    â†’ Semantic search (OpenAI embedding â†’ Pinecone)
    â†’ Combine & dedupe by chunk_id
```

**Key Point**: Claude passes `source_id` to search, but receives `chunk_id` in results. Citations must use `chunk_id`, NOT `source_id`.

### Chunk-Based Citations

Citations use chunk_ids for precise references to specific content sections.

**Citation Format**: `[[cite:CHUNK_ID]]`
**Chunk ID Format**: `{source_id}_page_{page}_chunk_{n}`
**Example**: `[[cite:abc123-def456_page_5_chunk_2]]`

**API Endpoint**: `GET /projects/{id}/citations/{chunk_id}`
Returns chunk content for tooltip/popover display.

**Flow**:
```
Claude response: "Information here [[cite:abc123_page_5_chunk_2]]"
       â†“
Frontend parses â†’ extracts chunk_id
       â†“
Hover â†’ GET /api/v1/projects/{id}/citations/{chunk_id}
       â†“
Backend loads chunk file â†’ returns content + metadata
       â†“
Tooltip shows: "Source Name - Page 5, Section 2" + content
```

### Debug Logging
Each API call logged to `data/projects/{id}/chats/{chat_id}/api_N.json` with full request/response for debugging.

## Claude API Response Parsing

Centralized parsing via `utils/claude_parsing_utils.py`. Clean separation of concerns:

```
claude_service.py (API call) â†’ returns raw {content_blocks, stop_reason, usage, model}
         â†“
claude_parsing_utils.py (parse response)
   - is_tool_use(response) / is_end_turn(response)
   - extract_text(response)
   - extract_tool_use_blocks(response)
   - build_tool_result_content(results)
   - serialize_content_blocks(blocks)
         â†“
message_service.py (store if needed - pure CRUD)
```

**Tool use flow**: `stop_reason: "tool_use"` â†’ extract tool_use blocks â†’ execute â†’ build tool_result content â†’ send back with matching IDs.

## Cost Tracking

Per-project API cost tracking. Pricing: Sonnet ($3/$15 per 1M in/out), Haiku ($1/$5). All services pass `project_id` to `claude_service.send_message()`. Costs stored in `project.json`, displayed in ProjectHeader with tooltip breakdown.

## Memory System

**Tool-based memory** for persistent context across conversations.

- **User Memory** (`data/user_memory.json`): Global preferences across all projects
- **Project Memory** (`data/projects/{id}/memory.json`): Project-specific context

**Flow**: Claude calls `store_memory` â†’ returns immediately (non-blocking) â†’ background task uses Haiku to merge new + existing memory (max 150 tokens) â†’ saved to JSON â†’ included in future system prompts via `context_loader.py`.

## Tier Configuration

Centralized rate limiting in `app/config/tier_loader.py`. Set via `ANTHROPIC_TIER` in .env (1-4).

| Tier | Workers | Pages/min | Use Case |
|------|---------|-----------|----------|
| 1 | 4 | 10 | Free tier |
| 2 | 16 | 100 | Standard |
| 3 | 24 | 200 | Pro |
| 4 | 80 | 1500 | Enterprise/Demos |

**Tier 4 Optimization**: Workers can be high (80) because PDF/PPTX processing is I/O-bound (waiting for API), not CPU-bound. The 4000 RPM limit with 5-page batches theoretically supports ~20,000 pages/min, but output token limits (~800K/min) cap practical throughput.

## AI Service Standard Pattern

All AI services (`ai_services/`), AI agents (`ai_agents/`), and tool executors (`tool_executors/`) must follow this standardized pattern for consistency and maintainability.

### Required Steps (Mandatory)

```
1. CONFIGURATION LOADING
   â”œâ”€â”€ prompt_loader.get_prompt_config("service_name")  # System prompt, model, temperature, max_tokens
   â”œâ”€â”€ tool_loader.load_tool("category", "tool_name")   # Tool definitions (if using tools)
   â””â”€â”€ get_anthropic_config()                           # Tier config (workers, rate limits)

2. PATH MANAGEMENT
   â””â”€â”€ path_utils.get_*_dir(project_id)                 # Use path_utils for ALL directory access
       â”œâ”€â”€ get_processed_dir()     # For output files
       â”œâ”€â”€ get_raw_dir()           # For input files
       â”œâ”€â”€ get_chunks_dir()        # For chunked text
       â””â”€â”€ get_chats_dir()         # For chat files

3. API CALL
   â””â”€â”€ claude_service.send_message(                     # Thin wrapper, returns raw response
           messages, system_prompt, model,
           max_tokens, temperature, tools,
           tool_choice, project_id                      # project_id required for cost tracking
       )

4. RESPONSE PARSING
   â””â”€â”€ claude_parsing_utils.*                           # Centralized parsing utilities
       â”œâ”€â”€ is_tool_use(response)                        # Check if tool was called
       â”œâ”€â”€ is_end_turn(response)                        # Check if conversation ended
       â”œâ”€â”€ extract_text(response)                       # Get text content
       â”œâ”€â”€ extract_tool_use_blocks(response)            # Get tool call details
       â”œâ”€â”€ extract_tool_inputs(response, tool_name)     # Get inputs for specific tool
       â””â”€â”€ build_tool_result_content(results)           # Build tool result message
```

### Optional Steps (As Needed)

```
FOR BATCHED PROCESSING (PDF, PPTX):
â”œâ”€â”€ batching_utils.create_batches(items, DEFAULT_BATCH_SIZE)
â”œâ”€â”€ batching_utils.get_batch_info(items, batch_size)
â””â”€â”€ DEFAULT_BATCH_SIZE = 5                              # Standard batch size

FOR RATE-LIMITED APIs:
â”œâ”€â”€ rate_limit_utils.RateLimiter(requests_per_minute)
â””â”€â”€ rate_limiter.wait_if_needed()                       # Call before each API request

FOR PARALLEL PROCESSING:
â”œâ”€â”€ ThreadPoolExecutor(max_workers=tier_config["max_workers"])
â”œâ”€â”€ task_service.is_target_cancelled(source_id)         # Check for user cancellation
â””â”€â”€ Cooperative cancellation pattern                    # Raise CancelledException

FOR BINARY DATA:
â””â”€â”€ encoding_utils.encode_bytes_to_base64(data)         # Base64 encoding for API

FOR FILE-TYPE SPECIFIC:
â”œâ”€â”€ pdf_utils.get_page_count(), get_all_page_bytes()    # PDF operations
â”œâ”€â”€ docx_utils.extract_text()                           # DOCX extraction
â””â”€â”€ pptx_utils.convert_to_pdf()                         # PPTX to PDF conversion
```

### Service Template

```python
"""
Service Name - Brief description of what this service does.
"""
from app.config import prompt_loader, tool_loader, get_anthropic_config
from app.services.integrations.claude import claude_service
from app.utils import claude_parsing_utils
from app.utils.path_utils import get_processed_dir
from app.utils.rate_limit_utils import RateLimiter  # If rate limiting needed
from app.utils.batching_utils import create_batches, DEFAULT_BATCH_SIZE  # If batching needed
from app.services.data_services import message_service # if message storage required


class ServiceName:
    def __init__(self):
        self._tool_definition = None  # Lazy load tools

    def _load_tool_definition(self):
        if self._tool_definition is None:
            self._tool_definition = tool_loader.load_tool("category", "tool_name")
        return self._tool_definition

    def process(self, project_id: str, ...) -> Dict[str, Any]:
        # 1. Load configurations
        prompt_config = prompt_loader.get_prompt_config("service_name")
        tool_def = self._load_tool_definition()
        tier_config = get_anthropic_config()

        # 2. Create rate limiter (if needed)
        rate_limiter = RateLimiter(tier_config["pages_per_minute"])

        # 3. Get paths
        output_dir = get_processed_dir(project_id)

        # 4. Process with rate limiting
        rate_limiter.wait_if_needed()
        response = claude_service.send_message(
            messages=messages,
            system_prompt=prompt_config["system_prompt"],
            model=prompt_config["model"],
            max_tokens=prompt_config["max_tokens"],
            temperature=prompt_config["temperature"],
            tools=[tool_def],
            project_id=project_id
        )

        # 5. Parse response
        tool_inputs = claude_parsing_utils.extract_tool_inputs(response, "tool_name")

        return {"success": True, "data": tool_inputs}


# Singleton instance
service_name = ServiceName()
```

### What NOT to Do

- **Never** duplicate configuration loading logic - use `prompt_loader`, `tool_loader`, `tier_loader`
- **Never** hardcode paths - use `path_utils` functions
- **Never** parse Claude responses manually - use `claude_parsing_utils`
- **Never** implement custom rate limiting - use `RateLimiter` class
- **Never** implement custom batching - use `create_batches()` utility
- **Never** skip `project_id` in API calls - needed for cost tracking
